# XGBoost

[toc]

## GBDT

GBDT的原理，就是所有**弱分类器的结果相加等于预测值**，后下一个弱分类器去拟合误差函数对预测值的残差。

当损失函数为均方时，可以用负梯度近似残差：

[![img](https://camo.githubusercontent.com/117ab1cc778f54c47fc46affd3c9628214929234/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353231343936323033343934343633382e676966)](https://camo.githubusercontent.com/117ab1cc778f54c47fc46affd3c9628214929234/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353231343936323033343934343633382e676966)

[![img](https://camo.githubusercontent.com/8fc85508bff8e559f631ffa4a2816876ce6f4f43/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353231343936323431363637303937332e676966)](https://camo.githubusercontent.com/8fc85508bff8e559f631ffa4a2816876ce6f4f43/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353231343936323431363637303937332e676966)

## 目标函数

![img](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64153438580139159593.png)

XGBoost的**核心算法思想**：

1. 不断地添加树，不断地进行特征分裂来生长一棵树，每次添加一个树，其实是学习一个新函数**f(x)**，去拟合上次预测的残差。
2. 当我们训练完成得到k棵树，我们要预测一个样本的分数，其实就是根据这个样本的特征，在每棵树中会落到对应的一个叶子节点，每个叶子节点就对应一个分数
3. 最后只需要将每棵树对应的分数加起来就是该样本的预测值。

![img](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64153438657261833493.png)

选取一个 f 来使得我们的目标函数尽量最大地降低。这里 f 可以使用泰勒展开公式近似。

![img](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415343865867530120.png)

## 正则项

正则项与树的复杂度有关，分为两个部分：

* 树里叶子节点的个数
* 叶子节点得分的模平方

![img](https://img-blog.csdnimg.cn/20190225115350935.png)

## 防止过拟合

* **Shrinkage** 在每次迭代中对树的每个叶子节点的分数乘上一个缩减权重"n"，这可以使得每一颗树的影响力都不会太大，留下更大的空间给后面生成的树去优化模型。

* **Column Sampling** 类似于随机森林中的选取部分特征进行建树。可分为：一种是按层随机采样，另一种是随机选择特征。

  

